{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Step 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd6t0uFzuR4X"
      },
      "source": [
        "### Installing Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6apGVff5h4ca"
      },
      "source": [
        "Install Dependencies:\n",
        "\n",
        "\n",
        "1.   Java 8\n",
        "2.   Apache Spark with hadoop\n",
        "3.   Findspark (used to locate the spark in the system)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt7ZS1_wGgjn"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3x0ZRLxjMVr"
      },
      "source": [
        "Set Environment Variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdOOq4twHN1K"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR1zLBk1998Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "c0d792bd-5e9d-4361-910e-869428f19cd6"
      },
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Property used to format output tables better\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "spark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://94d0699a7ac5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fbd1af0e8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79s4smf6E3Uv"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEMnbLQp5rcc"
      },
      "source": [
        "Loading data as RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD9N_zagDIZk"
      },
      "source": [
        "data = spark.sparkContext.textFile('input.txt')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "vY98h-4TF-aI",
        "outputId": "627af48a-6060-4609-d8bc-d584509a2b29"
      },
      "source": [
        "# sample first item of data\n",
        "data.first()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Games are a fun way to get people involved and learning in a happy environment and get them to work on concepts and tactics without them knowing it a lot of the time. Because of this, these games were perfect in a class on negotiation and persuasion because it loosened people up and allowed them to learn in a fun environment. The games used in this class were reinforcing the concepts we talked about and got familiar with in the lectures, but the games were a safe place where we could give them a spin, test drive these persuasion tactics on our peers. With this we were able to make the connection between theory and application of concepts which have no use on paper. This class was about learning how to use language and framing to the extent where you make people think they want what you want, get to identify with your subject and get them on your level before you persuade them to act, and such concepts are great in theory but the application of them takes some practice and with mastery, as Rufo has shown us, anything really is possible. There were five total games: Bullshit, Car salesman, Werewolf, XY, and the final game which goes by many names. For the purposes of this paper it will be referred to as “Fuck Your Buddy”. There was another “game” called I Win You Lose but since that was really only a precursor the the debate off I didn’t really count it. There were several tactics used in each of the games and a lot of these overlapped. The ones I’m going to discuss are lying, identification, and collaboration. These three are some of the strongest persuasion tactics I have found when playing these games, and often times lying and admitting it straight away can help me in the long run, knowing people and what they like so I can tell them what they want to hear, and “teaming up” with people that I know I’m stronger at persuading than can make it easier to get rid of the competition. The combination of all of these tactics have allowed me to excel at the games played in class and have a strong grasp of the theory that the games are focused on. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9LnRyWhGBla",
        "outputId": "ffcd5d19-fab6-4736-cc89-69563cfbd946"
      },
      "source": [
        "# count all data\n",
        "data.count()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9PkJ61OGLH6",
        "outputId": "622d41c9-7782-4b56-a6c6-e66eb6ad3c87"
      },
      "source": [
        "# get number of partitions (logical division of data stored on a node in cluster)\n",
        "data.getNumPartitions()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzS78BiYGvs-"
      },
      "source": [
        "### RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h5D-OE_m9yU"
      },
      "source": [
        "#### Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5vBcvci6647"
      },
      "source": [
        "Count total words in RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRsm_U7SGweb"
      },
      "source": [
        "import string\n",
        "\n",
        "# mapper function\n",
        "def word_counter(line):\n",
        "  # remove punctuation\n",
        "  line = line.replace('“',r'\"')\n",
        "  line = line.replace('”',r'\"')\n",
        "  line = line.replace('’',r\"'\")\n",
        "  line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  return len(line.split())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMlq3ZphHyGb",
        "outputId": "2b1f059f-2bfc-432f-e530-3fcc48e7564d"
      },
      "source": [
        "from operator import add\n",
        "\n",
        "cnt = data.map(word_counter).reduce(add)\n",
        "print('total number of words: ', cnt)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total number of words:  5004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdyjQwO27W7Z"
      },
      "source": [
        "Count words frequecy in RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XjAJqeQH1AB"
      },
      "source": [
        "import string\n",
        "\n",
        "# mapper function\n",
        "def word_freq(line):\n",
        "  # remove punctuation\n",
        "  line = line.replace('“',r'\"')\n",
        "  line = line.replace('”',r'\"')\n",
        "  line = line.replace('’',r\"'\")\n",
        "  line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # normalization\n",
        "  line = line.lower()\n",
        "  \n",
        "  word_set = set(line.split())\n",
        "  output = {}\n",
        "  for w in word_set:\n",
        "    output[w] = line.count(w)\n",
        "  return output\n",
        "\n",
        "# reducer function\n",
        "def reducer(d1, d2):\n",
        "  output = {**d1, **d2}\n",
        "  intersection = d1.keys() & d2.keys()\n",
        "  for key in intersection:\n",
        "    output[key] = d1[key] + d2[key]\n",
        "  return output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajvWeBq5KncQ"
      },
      "source": [
        "import json\n",
        "\n",
        "res = data.map(word_freq).reduce(reducer)\n",
        "\n",
        "with open('p1.txt', 'w') as f:\n",
        "  for key, value in res.items():\n",
        "    f.write('{0}, {1}\\n'.format(key, value))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BukZPZUenDDo"
      },
      "source": [
        "#### Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcTbEqCq71JS"
      },
      "source": [
        "Count total words that start with `m\\M` letter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "men6IMINKqJq"
      },
      "source": [
        "import string\n",
        "\n",
        "# mapper function\n",
        "def mword_counter(line):\n",
        "  # remove punctuation\n",
        "  line = line.replace('“',r'\"')\n",
        "  line = line.replace('”',r'\"')\n",
        "  line = line.replace('’',r\"'\")\n",
        "  line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # normalization\n",
        "  line = line.lower()\n",
        "  \n",
        "  word_list = line.split()\n",
        "  mword_list = list(filter(lambda w: w.startswith('m'), word_list))\n",
        "  return len(mword_list)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSVQkLKQQcGh",
        "outputId": "80c52a31-c0e3-4551-96b1-268fd8940f14"
      },
      "source": [
        "from operator import add\n",
        "\n",
        "cnt = data.map(mword_counter).reduce(add)\n",
        "print('total number of words start with m/M: ', cnt)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total number of words start with m/M:  150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H24X-3tDnG-t"
      },
      "source": [
        "#### Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZvg5nNb8hSo"
      },
      "source": [
        "Count words with `length=5` that don't start with vowel letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lghtjn_NQd_A"
      },
      "source": [
        "import string\n",
        "\n",
        "vowel = ['a', 'e', 'i', 'o', 'u']\n",
        "\n",
        "# mapper function\n",
        "def mapper(line):\n",
        "  # remove punctuation\n",
        "  line = line.replace('“',r'\"')\n",
        "  line = line.replace('”',r'\"')\n",
        "  line = line.replace('’',r\"'\")\n",
        "  line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # normalization\n",
        "  line = line.lower()\n",
        "\n",
        "  word_list = line.split()\n",
        "  # filter by word length\n",
        "  word_list = list(filter(lambda w: len(w)==5, word_list))\n",
        "  # filter by start letter\n",
        "  word_list = list(filter(lambda w: w.lower()[0] not in vowel, word_list))\n",
        "  return len(word_list)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX5KKlglUGIZ",
        "outputId": "6aeb2c98-0b40-4cd4-8927-15e051e0c1dc"
      },
      "source": [
        "from operator import add\n",
        "\n",
        "cnt = data.map(mapper).reduce(add)\n",
        "print('total number of words with 5 length and not start with vowel: ', cnt)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total number of words with 5 length and not start with vowel:  492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obEHwb9lnKdh"
      },
      "source": [
        "#### Part 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhnhUOrr-EL_"
      },
      "source": [
        "Filter items from stop words and non-alphabatic letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3CptnqBVpyT"
      },
      "source": [
        "import json\n",
        "\n",
        "# word frequency dict\n",
        "wf = {}\n",
        "with open('p1.txt') as f:\n",
        "    for line in f:\n",
        "      key, value = line.split(',')\n",
        "      value = int(value)\n",
        "      wf[key] = value\n",
        "\n",
        "# stop words (top 10 percent in frequency ranking)\n",
        "sw = dict(sorted(wf.items(), key=lambda item: item[1], reverse=True)[:len(wf) // 10])\n",
        "sw = list(sw.keys())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvSm0IGRaBss"
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "# mapper function\n",
        "def mapper(line):\n",
        "  # remove punctuation\n",
        "  line = line.replace('“',r'\"')\n",
        "  line = line.replace('”',r'\"')\n",
        "  line = line.replace('’',r\"'\")\n",
        "  line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # normalization\n",
        "  line = line.lower()\n",
        "\n",
        "  # filter by stop words\n",
        "  filtered_line = ' '.join([w for w in line.split() if not w in sw])\n",
        "  # filter by non-alphabetic letters\n",
        "  filtered_line = re.sub(r'\\W+', ' ', filtered_line)\n",
        "  return filtered_line"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9frDsc6cjLz"
      },
      "source": [
        "res = data.map(mapper).collect()\n",
        "\n",
        "with open('p4.txt', 'w') as f:\n",
        "  for line in res:\n",
        "    f.write('{}\\n'.format(line))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-qX-pw0nOfG"
      },
      "source": [
        "#### Part 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTk3bHWd-eX0"
      },
      "source": [
        "Count bigrams frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g31SnyREe_zb"
      },
      "source": [
        "import string\n",
        "\n",
        "# mapper function\n",
        "def mapper(line):\n",
        "  # remove punctuation\n",
        "  line = line.replace('“',r'\"')\n",
        "  line = line.replace('”',r'\"')\n",
        "  line = line.replace('’',r\"'\")\n",
        "  line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # normalization\n",
        "  line = line.lower()\n",
        "\n",
        "  bigrams = [bg for bg in zip(line.split()[:-1], line.split()[1:])]\n",
        "  output = {bg: bigrams.count(bg) for bg in set(bigrams)}\n",
        "  return output\n",
        "\n",
        "# reducer function\n",
        "def reducer(d1, d2):\n",
        "  output = {**d1, **d2}\n",
        "  intersection = d1.keys() & d2.keys()\n",
        "  for key in intersection:\n",
        "    output[key] = d1[key] + d2[key]\n",
        "  return output"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSbrFH4Nift0"
      },
      "source": [
        "res = data.map(mapper).reduce(reducer)\n",
        "\n",
        "res = dict(sorted(res.items(), key=lambda item: item[1], reverse=True))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NMD20yI_p28"
      },
      "source": [
        "with open('p5.txt', 'w') as f:\n",
        "  for key, value in res.items():\n",
        "    f.write('{0}, {1}\\n'.format(key, value))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CmUQSiaiiJ_"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}